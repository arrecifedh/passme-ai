üü¢ Introducci√≥n
¬øPor qu√© hablar de descolonizar la IA ahora?
La inteligencia artificial (IA) avanza r√°pidamente en Am√©rica Latina y el Caribe. Gobiernos, empresas y organismos internacionales promueven su adopci√≥n como una soluci√≥n eficiente para mejorar servicios p√∫blicos, optimizar pol√≠ticas sociales, fortalecer la seguridad, modernizar la justicia o impulsar el desarrollo econ√≥mico. Sin embargo, este despliegue acelerado ocurre, en la mayor√≠a de los casos, sin marcos regulatorios s√≥lidos, sin datos representativos de nuestras poblaciones y sin una reflexi√≥n profunda sobre sus impactos sociales, pol√≠ticos y culturales.
Lejos de ser una tecnolog√≠a neutral, la IA es una infraestructura de poder. Los sistemas algor√≠tmicos incorporan decisiones humanas, valores impl√≠citos y supuestos hist√≥ricos que reflejan, y muchas veces amplifican, las desigualdades existentes. Cuando estos sistemas se dise√±an, entrenan e implementan desde contextos ajenos a la realidad latinoamericana, el riesgo no es √∫nicamente t√©cnico: es estructural. En este escenario, la IA puede convertirse en una nueva forma de colonialidad tecnol√≥gica, reproduciendo dependencias hist√≥ricas, concentrando poder y profundizando exclusiones.
Hablar de descolonizar la inteligencia artificial no es una met√°fora acad√©mica ni una consigna abstracta. Es una necesidad urgente frente a un modelo dominante de desarrollo tecnol√≥gico que posiciona a Am√©rica Latina como consumidora de tecnolog√≠as importadas, pero rara vez como productora de conocimiento, marcos √©ticos o infraestructuras propias. La mayor√≠a de los modelos de IA que se utilizan en la regi√≥n han sido entrenados con datos del Norte Global, en idiomas hegem√≥nicos, bajo l√≥gicas culturales, jur√≠dicas y econ√≥micas que no reflejan nuestras realidades sociales, territoriales ni hist√≥ricas.
Este desbalance tiene consecuencias concretas. Sistemas automatizados de focalizaci√≥n social que excluyen a poblaciones vulnerables; tecnolog√≠as de vigilancia que afectan de forma desproporcionada a comunidades racializadas; modelos ling√º√≠sticos que invisibilizan lenguas ind√≠genas; algoritmos de reconocimiento facial con altas tasas de error en mujeres y personas afrodescendientes; plataformas digitales que refuerzan estereotipos de g√©nero y sexualidad. Estos no son errores aislados: son s√≠ntomas de un dise√±o tecnol√≥gico que no ha sido pensado desde el Sur Global ni para sus pueblos.
La relaci√≥n entre IA y derechos humanos es, por tanto, central. La adopci√≥n de sistemas algor√≠tmicos en √°mbitos como la seguridad social, la justicia, la migraci√≥n, la salud o la educaci√≥n incide directamente sobre derechos fundamentales: igualdad y no discriminaci√≥n, privacidad, debido proceso, acceso a servicios p√∫blicos, autodeterminaci√≥n informativa y participaci√≥n democr√°tica. Sin una gobernanza adecuada, la IA puede consolidar pr√°cticas de exclusi√≥n automatizada, dificultar los mecanismos de rendici√≥n de cuentas y debilitar el control ciudadano sobre decisiones que afectan la vida cotidiana de millones de personas.
Desde Am√©rica Latina, diversas autoras, organizaciones sociales, comunidades ind√≠genas, colectivos feministas y centros de investigaci√≥n vienen advirtiendo que no basta con adaptar marcos √©ticos globales. Es necesario construir enfoques situados, anclados en las experiencias hist√≥ricas de la regi√≥n, en sus luchas por la justicia social, en la diversidad cultural y ling√º√≠stica, y en una comprensi√≥n cr√≠tica del poder. Descolonizar la IA implica cuestionar qui√©n dise√±a la tecnolog√≠a, con qu√© datos, para qu√© fines y bajo qu√© relaciones de poder. Implica tambi√©n disputar la idea de que el progreso tecnol√≥gico es inevitable y neutral, y abrir el debate sobre qu√© tipo de tecnolog√≠a queremos y para qui√©n.
Este documento se inscribe en esa disputa. A lo largo de los siguientes cap√≠tulos, organizados en torno a las cinco semanas de la campa√±a de enero, se propone un recorrido que va del diagn√≥stico a la acci√≥n: qu√© significa descolonizar la IA, c√≥mo identificar sistemas algor√≠tmicos coloniales, a qui√©nes da√±an de forma desproporcionada, qu√© alternativas se est√°n construyendo desde la regi√≥n y c√≥mo empezar a transformar estas pr√°cticas en pol√≠ticas p√∫blicas, marcos regulatorios y procesos de dise√±o tecnol√≥gico m√°s justos.
Este trabajo forma parte del esfuerzo m√°s amplio de Arrecife, una organizaci√≥n que trabaja en la intersecci√≥n entre derechos humanos, datos y tecnolog√≠a en Am√©rica Latina. Desde una perspectiva feminista, interseccional y regional, Arrecife busca contribuir a debates p√∫blicos informados, generar evidencia cr√≠tica y fortalecer capacidades para que la tecnolog√≠a no sea una nueva herramienta de exclusi√≥n, sino un medio para la justicia algor√≠tmica y la autodeterminaci√≥n colectiva.
Descolonizar la IA no significa rechazar la tecnolog√≠a. Significa reapropiarla, disputarla y reconstruirla desde nuestras realidades. En un momento en que la inteligencia artificial redefine aceleradamente las formas de gobernar, vigilar, clasificar y decidir, no intervenir tambi√©n es una decisi√≥n pol√≠tica. Este documento propone intervenir con mirada cr√≠tica, situada y colectiva.


üü© Semana 1 ‚Äî ¬øQu√© significa descolonizar la IA?
Cap√≠tulo 1: M√°s all√° de la met√°fora
Hablar de descolonizar la inteligencia artificial no es un ejercicio ret√≥rico ni una consigna abstracta. En Am√©rica Latina y el Caribe, la tecnolog√≠a ha sido hist√≥ricamente una herramienta de dominaci√≥n, introducida desde centros de poder externos para organizar territorios, clasificar poblaciones, extraer recursos y administrar desigualdades. La IA no rompe con esa historia: la contin√∫a bajo nuevas formas.
Desde esta regi√≥n, descolonizar la IA implica nombrar la colonialidad tecnol√≥gica y rechazar la idea de que los sistemas algor√≠tmicos son neutrales, universales o inevitables.
Colonialidad tecnol√≥gica vs. neutralidad t√©cnica
La narrativa dominante presenta a la IA como una tecnolog√≠a objetiva, guiada por datos y eficiencia. Sin embargo, como advierte Paola Ricaurte, los sistemas de datos y automatizaci√≥n est√°n profundamente atravesados por relaciones de poder, intereses econ√≥micos y l√≥gicas extractivas. La colonialidad tecnol√≥gica se manifiesta cuando los territorios del Sur Global son reducidos a fuentes de datos, campos de prueba o mercados de consumo, sin capacidad real de decisi√≥n sobre el dise√±o, uso o consecuencias de estas tecnolog√≠as.
Esta supuesta neutralidad t√©cnica oculta que la IA:
    ‚Ä¢ se entrena con datos hist√≥ricamente sesgados,
    ‚Ä¢ se dise√±a desde epistemolog√≠as ajenas a los contextos locales,
    ‚Ä¢ y se implementa sin mecanismos efectivos de rendici√≥n de cuentas para las comunidades afectadas.
Descolonizar la IA exige desmontar esta ficci√≥n de neutralidad y reconocer que toda tecnolog√≠a es pol√≠tica.
Epistemolog√≠as del Norte Global y exclusi√≥n del saber situado
La producci√≥n de conocimiento sobre IA contin√∫a concentr√°ndose en universidades, corporaciones y centros de investigaci√≥n del Norte Global. Esta hegemon√≠a epist√©mica define qu√© problemas importan, qu√© soluciones son v√°lidas y qu√© formas de conocimiento son consideradas leg√≠timas.
Desde una cr√≠tica decolonial, Silvia Rivera Cusicanqui ha advertido que muchas teor√≠as cr√≠ticas reproducen formas de colonialismo cuando hablan sobre el Sur sin partir desde el Sur. Aplicado a la IA, esto se traduce en modelos que ignoran las realidades sociales, ling√º√≠sticas, territoriales y culturales de Am√©rica Latina y el Caribe, mientras imponen categor√≠as universales que borran la diferencia.
Descolonizar la IA implica entonces recuperar el saber situado, reconocer la validez de conocimientos producidos desde comunidades ind√≠genas, afrodescendientes, feministas y populares, y rechazar la idea de que la innovaci√≥n solo fluye desde el Norte hacia el Sur.
La IA como infraestructura de poder
La inteligencia artificial no es solo software: es infraestructura material y pol√≠tica. Requiere centros de datos, energ√≠a, agua, minerales cr√≠ticos y marcos regulatorios favorables. En este sentido, la IA se inscribe en las mismas l√≥gicas extractivas que han marcado la historia de la regi√≥n.
Como se√±ala Maristella Svampa, las infraestructuras contempor√°neas reproducen formas de despojo cuando se imponen sin control democr√°tico ni consideraci√≥n por los territorios. La IA, desplegada sin soberan√≠a tecnol√≥gica, refuerza la dependencia econ√≥mica, profundiza asimetr√≠as y consolida nuevas formas de subordinaci√≥n digital.
Pensar la IA como infraestructura de poder permite entender que sus impactos no son solo t√©cnicos, sino sociales, ambientales y pol√≠ticos, y que afectan de manera desigual a quienes ya han sido hist√≥ricamente marginados.
Justicia algor√≠tmica y autodeterminaci√≥n regional
Frente a este escenario, las autoras feministas y decoloniales del Caribe y Am√©rica Latina proponen ir m√°s all√° de la ‚Äú√©tica de la IA‚Äù entendida como checklist t√©cnico. Yuderkys Espinosa Mi√±oso y Ochy Curiel han insistido en que no puede haber justicia sin cuestionar las estructuras coloniales, raciales y patriarcales que organizan el conocimiento y el poder.
Aplicado a la IA, esto implica:
    ‚Ä¢ reconocer que los sistemas algor√≠tmicos afectan de forma desproporcionada a mujeres, personas LGBTIQ+, pueblos ind√≠genas y comunidades racializadas;
    ‚Ä¢ rechazar soluciones tecnol√≥gicas que ‚Äúcorrigen sesgos‚Äù sin transformar las estructuras que los producen;
    ‚Ä¢ y avanzar hacia una justicia algor√≠tmica basada en la autodeterminaci√≥n de los pueblos, la soberan√≠a de datos y la participaci√≥n comunitaria real.
Descolonizar la IA no significa rechazar la tecnolog√≠a, sino reapropiarla pol√≠ticamente, disputar su sentido y reorientarla hacia la defensa de los derechos humanos y la justicia social en la regi√≥n.


üü® Semana 2 ‚Äî ¬øC√≥mo identificar IA colonial?
Cap√≠tulo 2: Se√±ales t√©cnicas, pol√≠ticas y sociales
Si descolonizar la IA es una tarea pol√≠tica y t√©cnica, identificar la IA colonial es el primer paso pr√°ctico. En Am√©rica Latina y el Caribe, muchos sistemas de inteligencia artificial no se presentan expl√≠citamente como herramientas de control o exclusi√≥n. Al contrario, suelen introducirse bajo discursos de eficiencia, modernizaci√≥n o innovaci√≥n p√∫blica. Sin embargo, sus efectos materiales revelan patrones claros de colonialidad tecnol√≥gica.
Este cap√≠tulo propone criterios concretos para diagnosticar cu√°ndo un sistema de IA reproduce desigualdades estructurales, concentraciones de poder y exclusiones hist√≥ricas.

1. Se√±ales t√©cnicas: cuando los datos ya vienen torcidos
Sesgos en datos: hist√≥ricos y t√©cnicos
Una de las se√±ales m√°s claras de IA colonial es el sesgo en los datos de entrenamiento. Estos sesgos no son errores accidentales, sino el reflejo de estructuras sociales desiguales.
    ‚Ä¢ Sesgo hist√≥rico: ocurre cuando los datos reproducen discriminaciones pasadas (raciales, de g√©nero, territoriales o de clase). Cuando estos datos se utilizan en sistemas de alto impacto, como justicia, seguridad o pol√≠ticas sociales, la IA automatiza la desigualdad.
    ‚Ä¢ Sesgo t√©cnico: surge de decisiones metodol√≥gicas durante la recolecci√≥n, limpieza o selecci√≥n de datos, como muestreos incompletos o proxies discriminatorios (por ejemplo, c√≥digo postal como sustituto de nivel socioecon√≥mico).
En Chile, el Registro Social de Hogares utiliz√≥ datos administrativos que excluyeron a personas trans y a convivientes del mismo sexo durante la pandemia, dej√°ndolas fuera de ayudas estatales. El problema no fue solo administrativo: fue algor√≠tmico y estructural.
En Colombia, el sistema SISB√âN IV ha sido criticado por codificar desigualdades hist√≥ricas de clase y territorio, con escasos mecanismos de apelaci√≥n humana para las personas afectadas.

2. Sesgo ling√º√≠stico digital: qui√©n puede ‚Äúhablar‚Äù con la IA
Otra se√±al t√©cnica clave para identificar la colonialidad algor√≠tmica es el sesgo ling√º√≠stico digital. Aunque muchos modelos de inteligencia artificial declaran incluir el ‚Äúespa√±ol‚Äù entre sus lenguas de funcionamiento, esto no implica que representen de manera justa la diversidad ling√º√≠stica de Am√©rica Latina y el Caribe. En la pr√°ctica, la mayor√≠a de estos sistemas operan sobre una versi√≥n estandarizada del idioma, definida y validada desde el Norte Global, que invisibiliza las particularidades culturales, territoriales y sociales de la regi√≥n.
Los modelos de lenguaje tienden a privilegiar el espa√±ol normativo, subrepresentando las variantes regionales y locales, y excluyendo casi por completo las lenguas ind√≠genas y cooficiales. Esta jerarquizaci√≥n ling√º√≠stica no es un efecto colateral menor, sino una decisi√≥n estructural que determina qui√©n puede interactuar con la tecnolog√≠a en condiciones de igualdad y qui√©n queda sistem√°ticamente fuera de sus beneficios.
El impacto de este sesgo es doble. Por un lado, las personas y comunidades que no dominan el idioma hegem√≥nico enfrentan barreras reales para acceder a servicios digitales, informaci√≥n p√∫blica o sistemas automatizados de toma de decisiones. Por otro, los saberes, cosmovisiones y formas de conocimiento expresadas en lenguas no dominantes quedan excluidas de los procesos de entrenamiento de la IA, reforzando una forma de epistemicidio digital. En este sentido, la colonialidad del lenguaje en la IA no es √∫nicamente t√©cnica: es profundamente epist√©mica, y reproduce relaciones hist√≥ricas de poder sobre qui√©n puede ‚Äúhablar‚Äù, ser escuchado y reconocido por la tecnolog√≠a.

3. Modelos importados sin competencia contextual
Am√©rica Latina y el Caribe son, en gran medida, consumidores netos de tecnolog√≠as de inteligencia artificial. Esto se traduce en la adopci√≥n de modelos desarrollados en otros contextos geopol√≠ticos, econ√≥micos y culturales, que son implementados en la regi√≥n sin procesos profundos de adaptaci√≥n a las realidades locales. Lejos de ser neutrales, estos sistemas arrastran supuestos, prioridades y marcos de decisi√≥n definidos fuera del territorio en el que operan.
Una inteligencia artificial de car√°cter colonial suele manifestarse a trav√©s del uso de modelos ‚Äúoff-the-shelf‚Äù que no incorporan ajustes culturales ni sociales, del desconocimiento del contexto territorial en su dise√±o y despliegue, y de una fuerte dependencia de infraestructuras, licencias propietarias y agendas tecnol√≥gicas externas. Estas condiciones limitan la capacidad de los actores locales para auditar, modificar o cuestionar el funcionamiento de los sistemas que afectan directamente a sus poblaciones.
La ausencia de competencia contextual ‚Äîentendida como una comprensi√≥n real y situada del entorno social, econ√≥mico y pol√≠tico‚Äî convierte a estos modelos en instrumentos de imposici√≥n m√°s que en herramientas orientadas a resolver problemas p√∫blicos. En este escenario, las decisiones tecnol√≥gicas se desplazan hacia centros de poder externos, mientras la regi√≥n pierde capacidad de agencia, autonom√≠a y soberan√≠a tecnol√≥gica, reproduciendo din√°micas hist√≥ricas de dependencia bajo una nueva forma de colonialidad digital.


4. Se√±ales pol√≠ticas: concentraci√≥n de poder y dependencia tecnol√≥gica
M√°s all√° del c√≥digo y de los aspectos t√©cnicos, la inteligencia artificial de car√°cter colonial puede identificarse observando qui√©n dise√±a, controla y gobierna la tecnolog√≠a. La colonialidad algor√≠tmica se manifiesta cuando el poder de decisi√≥n se concentra en un n√∫mero reducido de actores, generalmente grandes corporaciones tecnol√≥gicas transnacionales, mientras los Estados y las comunidades quedan relegados a un rol de usuarios pasivos de sistemas que no controlan ni comprenden plenamente.
Una se√±al pol√≠tica central de esta din√°mica es la proliferaci√≥n de contratos opacos entre gobiernos y proveedores privados de tecnolog√≠a, en los que los criterios de dise√±o, los modelos de datos y los mecanismos de toma de decisiones quedan protegidos por secretos comerciales. A esto se suma la ausencia de marcos √©ticos, normativos y regulatorios desarrollados desde contextos locales, lo que impide evaluar adecuadamente los impactos sociales, de g√©nero y territoriales de los sistemas automatizados. La dependencia de infraestructuras extranjeras para la provisi√≥n de servicios p√∫blicos cr√≠ticos ‚Äîcomo seguridad, bienestar social o identificaci√≥n‚Äî profundiza a√∫n m√°s la p√©rdida de soberan√≠a tecnol√≥gica y limita la capacidad estatal de garantizar derechos.
En Argentina, organizaciones como DataG√©nero han documentado c√≥mo la implementaci√≥n de sistemas automatizados sin perspectiva de g√©nero ni transparencia puede profundizar desigualdades preexistentes en el dise√±o y la ejecuci√≥n de pol√≠ticas p√∫blicas. Estos sistemas, al operar sin mecanismos claros de rendici√≥n de cuentas, terminan reproduciendo sesgos estructurales que afectan de manera desproporcionada a mujeres y diversidades.
En Brasil, el uso de tecnolog√≠as de reconocimiento facial ha evidenciado tasas de error significativamente m√°s altas en la identificaci√≥n de personas negras, lo que ha derivado en detenciones arbitrarias y en pr√°cticas de vigilancia desproporcionada. En estos casos, la tecnolog√≠a no solo falla desde el punto de vista t√©cnico, sino que se convierte en un dispositivo que vulnera derechos fundamentales, refuerza l√≥gicas raciales hist√≥ricas y legitima nuevas formas de control y violencia estatal.

5. Se√±ales sociales: qui√©n paga el costo de la automatizaci√≥n
La inteligencia artificial de car√°cter colonial no afecta a todas las personas por igual. Sus impactos se distribuyen de manera profundamente desigual, reproduciendo y amplificando las jerarqu√≠as hist√≥ricas de raza, g√©nero, clase y territorio. En Am√©rica Latina y el Caribe, los efectos m√°s da√±inos de la automatizaci√≥n recaen de forma sistem√°tica sobre pueblos ind√≠genas, comunidades afrodescendientes, personas LGBTIQ+, as√≠ como sobre poblaciones rurales y empobrecidas que ya enfrentan m√∫ltiples barreras de acceso a derechos y servicios.
Cuando un sistema automatizado genera exclusi√≥n persistente del acceso a programas sociales, incrementa la vigilancia sobre determinados cuerpos o territorios, o limita el ejercicio de derechos b√°sicos como la identidad, la movilidad o la protecci√≥n social, no se trata de errores aislados ni de fallas t√©cnicas menores. Estos efectos constituyen se√±ales sociales claras de colonialidad algor√≠tmica, en las que la tecnolog√≠a act√∫a como un nuevo mecanismo de selecci√≥n, control y castigo sobre poblaciones hist√≥ricamente marginadas.
En este contexto, la pregunta central no debe ser si la inteligencia artificial funciona de manera eficiente o precisa en t√©rminos t√©cnicos, sino para qui√©n funciona, qui√©n define sus criterios de √©xito y qui√©n asume los costos de sus decisiones. Analizar la automatizaci√≥n desde una perspectiva de justicia algor√≠tmica implica desplazar el foco del rendimiento del sistema hacia sus impactos materiales y simb√≥licos, visibilizando a las personas y comunidades que quedan sistem√°ticamente fuera del dise√±o, la gobernanza y los beneficios de la tecnolog√≠a.

‚úîÔ∏è Checklist pr√°ctico: ¬øestamos frente a IA colonial?
Este checklist permite una evaluaci√≥n inicial r√°pida:
Datos
    ‚Ä¢ ¬øLos datos reflejan desigualdades hist√≥ricas?
    ‚Ä¢ ¬øIncluyen diversidad racial, territorial, de g√©nero y ling√º√≠stica?
    ‚Ä¢ ¬øSe utilizan proxies que puedan generar discriminaci√≥n indirecta?
Lenguaje
    ‚Ä¢ ¬øEl sistema reconoce variantes locales del idioma?
    ‚Ä¢ ¬øIncluye o excluye lenguas ind√≠genas?
Modelo
    ‚Ä¢ ¬øEs importado sin adaptaci√≥n profunda?
    ‚Ä¢ ¬øExiste competencia contextual en el equipo que lo implementa?
Gobernanza
    ‚Ä¢ ¬øQui√©n controla la infraestructura y los datos?
    ‚Ä¢ ¬øExisten mecanismos de apelaci√≥n y rendici√≥n de cuentas?
    ‚Ä¢ ¬øHay participaci√≥n real de comunidades afectadas?
Impacto
    ‚Ä¢ ¬øEl sistema produce exclusi√≥n, vigilancia o da√±o desproporcionado?
    ‚Ä¢ ¬øRefuerza desigualdades existentes?
Si la mayor√≠a de estas respuestas es afirmativa, no estamos ante un fallo t√©cnico aislado, sino frente a un caso de IA colonial.

Cierre 
Identificar la IA colonial no busca frenar la innovaci√≥n, sino repolitizarla. Sin diagn√≥stico no hay transformaci√≥n posible. Este cap√≠tulo sienta las bases para el siguiente paso: c√≥mo intervenir, redise√±ar y gobernar la IA desde marcos de justicia, soberan√≠a y autodeterminaci√≥n regional, que ser√° el foco de los pr√≥ximos cap√≠tulos.


üüß Semana 3 ‚Äî ¬øA qui√©n da√±a la IA colonial?
Cap√≠tulo 3: Impactos desiguales y exclusi√≥n algor√≠tmica
La IA colonial no da√±a a todas las personas por igual. Al contrario: profundiza desigualdades preexistentes, amplificando relaciones hist√≥ricas de dominaci√≥n basadas en raza, g√©nero, clase, sexualidad y territorio. En Am√©rica Latina y el Caribe, estos impactos no son hipot√©ticos: ya est√°n ocurriendo en sistemas de vigilancia, pol√≠ticas sociales, control migratorio y servicios p√∫blicos digitalizados.
Este cap√≠tulo analiza qui√©nes pagan el costo de la automatizaci√≥n colonial y c√≥mo esos da√±os se manifiestan tanto en el plano material como en el simb√≥lico.

1. Pueblos ind√≠genas y afrodescendientes: datos sin consentimiento, vigilancia sin protecci√≥n
Uno de los impactos m√°s graves de la inteligencia artificial de car√°cter colonial recae sobre los pueblos ind√≠genas y las comunidades afrodescendientes, hist√≥ricamente tratadas como objetos de extracci√≥n. Si en el pasado esta l√≥gica se expres√≥ a trav√©s del despojo de territorios y recursos naturales, hoy se reproduce mediante la extracci√≥n masiva de datos, conocimientos, im√°genes y pr√°cticas culturales, sin reconocimiento ni control por parte de las comunidades afectadas.
En el plano material, estos da√±os se manifiestan a trav√©s de la recolecci√≥n de datos sin consentimiento libre, previo e informado, as√≠ como del uso de im√°genes, voces e informaci√≥n cultural para entrenar modelos de inteligencia artificial sin mecanismos de retorno, beneficio o gobernanza comunitaria. A esto se suma la implementaci√≥n de tecnolog√≠as de vigilancia ‚Äîcomo drones, sistemas biom√©tricos o reconocimiento facial‚Äî en territorios ind√≠genas, muchas veces justificadas bajo discursos de ‚Äúseguridad‚Äù, ‚Äúcontrol fronterizo‚Äù o ‚Äúprotecci√≥n ambiental‚Äù, pero sin garant√≠as efectivas de derechos ni participaci√≥n local.
En pa√≠ses como Brasil, por ejemplo, se han documentado despliegues de tecnolog√≠as de monitoreo en territorios ind√≠genas sin marcos claros de gobernanza comunitaria, mientras que esas mismas comunidades contin√∫an enfrentando carencias estructurales en conectividad, acceso a servicios de salud digital y provisi√≥n de servicios b√°sicos. Esta asimetr√≠a evidencia que la tecnolog√≠a no se introduce para fortalecer derechos, sino para ampliar capacidades de control sobre territorios y cuerpos hist√≥ricamente marginados.
Los da√±os no son √∫nicamente materiales, sino tambi√©n profundamente simb√≥licos y epist√©micos. Los sistemas de inteligencia artificial suelen invisibilizar los saberes ind√≠genas, reduciendo formas complejas de conocimiento a categor√≠as t√©cnicas irrelevantes o directamente excluy√©ndolas de los procesos de dise√±o. Al mismo tiempo, las identidades colectivas son traducidas en ‚Äúvariables de riesgo‚Äù, m√©tricas de vulnerabilidad o perfiles de vigilancia, reforzando estigmas hist√≥ricos. De este modo, la colonialidad epist√©mica se perpet√∫a: los pueblos ind√≠genas y afrodescendientes son utilizados como fuente de datos, pero permanecen excluidos como sujetos de decisi√≥n, definici√≥n y control sobre las tecnolog√≠as que impactan directamente en sus vidas.

2. Comunidades LGBTIQ+: sobre-vigiladas y sub-protegidas
Las personas LGBTIQ+ enfrentan una paradoja central dentro de la inteligencia artificial de car√°cter colonial: son poblaciones intensamente vigiladas por los sistemas automatizados, pero al mismo tiempo permanecen insuficientemente protegidas frente a la discriminaci√≥n, la exclusi√≥n y la violencia institucional. Esta contradicci√≥n revela c√≥mo la IA no opera como una herramienta neutral, sino como un dispositivo que reproduce jerarqu√≠as morales, normativas y pol√≠ticas preexistentes.
En el plano material, los da√±os se expresan en sistemas de moderaci√≥n autom√°tica de contenidos que censuran de manera desproporcionada publicaciones vinculadas a educaci√≥n sexual, activismo LGBTIQ+, lenguaje reapropiado o expresiones identitarias no normativas, mientras permiten la circulaci√≥n de discursos de odio y violencia simb√≥lica. Esta asimetr√≠a no es accidental: responde a modelos entrenados con marcos morales conservadores y datasets que patologizan la diversidad sexual y de g√©nero.
Asimismo, muchas bases de datos estatales y sistemas administrativos no reconocen identidades trans, no binarias ni configuraciones familiares diversas. Esta omisi√≥n t√©cnica tiene consecuencias materiales directas, ya que excluye a estas poblaciones del acceso a programas sociales, servicios de salud, empleo y mecanismos de protecci√≥n estatal. A ello se suma el riesgo creciente del uso de sistemas de inteligencia artificial para la supuesta ‚Äúdetecci√≥n‚Äù de orientaci√≥n sexual o identidad de g√©nero, una pr√°ctica sin sustento cient√≠fico y con un alto potencial de persecuci√≥n, criminalizaci√≥n y violencia, especialmente en contextos de fragilidad democr√°tica o avance de discursos autoritarios.
En Argentina, colectivos feministas y de diversidad han documentado c√≥mo la ausencia de variables relacionadas con la orientaci√≥n sexual y la identidad de g√©nero (OSIG) en sistemas p√∫blicos produce exclusi√≥n sistem√°tica en pol√≠ticas de salud, empleo y protecci√≥n social. La falta de dise√±o inclusivo no solo invisibiliza a estas poblaciones, sino que reproduce activamente desigualdades estructurales a trav√©s de decisiones automatizadas.
Los da√±os tambi√©n operan en un nivel simb√≥lico profundo. Los sistemas de IA reproducen y refuerzan la cis-heteronormatividad como norma, present√°ndola como el est√°ndar neutral a partir del cual se definen desviaciones. Esto da lugar a procesos de patologizaci√≥n digital de identidades disidentes y al refuerzo de narrativas que asocian la diversidad sexual y de g√©nero con riesgo, desviaci√≥n o amenaza. De este modo, la inteligencia artificial no solo excluye, sino que legitima simb√≥licamente formas contempor√°neas de violencia estructural contra las comunidades LGBTIQ+.

3. Mujeres y programas sociales: automatizar la pobreza
La automatizaci√≥n de las pol√≠ticas sociales ha tenido un impacto particularmente severo sobre las mujeres, y de manera a√∫n m√°s profunda sobre aquellas que se encuentran en situaciones de pobreza, pertenecen a grupos racializados o son jefas de hogar. Lejos de corregir desigualdades estructurales, muchos sistemas automatizados de gesti√≥n social han tendido a reproducirlas y amplificarlas, incorporando en su dise√±o supuestos morales y econ√≥micos que individualizan la pobreza y despolitizan sus causas.
En el plano material, los da√±os se manifiestan a trav√©s de sistemas de focalizaci√≥n que clasifican a las personas seg√∫n categor√≠as de ‚Äúriesgo‚Äù o ‚Äúmerecimiento‚Äù basadas en datos incompletos, desactualizados o atravesados por sesgos hist√≥ricos de g√©nero, clase y raza. Estas herramientas determinan el acceso a beneficios sociales mediante l√≥gicas algor√≠tmicas opacas que, con frecuencia, generan exclusiones autom√°ticas sin ofrecer mecanismos claros, accesibles y efectivos de apelaci√≥n o revisi√≥n humana. Como resultado, errores t√©cnicos o inconsistencias en los datos pueden traducirse directamente en la p√©rdida de ingresos, alimentos o servicios b√°sicos.
Adem√°s, las mujeres se encuentran sobrerrepresentadas en bases de datos de vigilancia social, especialmente en programas que monitorean el cumplimiento de condicionalidades asociadas a transferencias monetarias o ayudas estatales. Esta sobreexposici√≥n refuerza un modelo de control que coloca a las mujeres ‚Äîen particular a las madres‚Äî como principales responsables del bienestar familiar, sometiendo su vida cotidiana a un escrutinio permanente por parte del Estado.
En Chile, durante la pandemia de COVID-19, los sistemas digitales dise√±ados para el acceso a ayudas estatales dejaron fuera a miles de mujeres debido a modelos familiares normativos y a la utilizaci√≥n de datos desactualizados. Entre las personas excluidas se encontraban mujeres trans y mujeres que no encajaban en las configuraciones familiares tradicionales previstas por los algoritmos, lo que evidenci√≥ c√≥mo la automatizaci√≥n puede operar como un mecanismo de exclusi√≥n estructural cuando no incorpora perspectivas de g√©nero y diversidad.
En el plano simb√≥lico, estos sistemas contribuyen a la construcci√≥n de la pobreza como una falla individual, desplazando la responsabilidad de las condiciones estructurales hacia las personas afectadas. Se refuerza as√≠ el estereotipo de la ‚Äúmujer asistida‚Äù como objeto de control, sospecha y disciplinamiento, legitimando la intervenci√≥n constante sobre su vida privada. De este modo, la vigilancia sobre la vida cotidiana femenina se normaliza como una pr√°ctica t√©cnica y neutral, cuando en realidad constituye una forma contempor√°nea de violencia institucional mediada por algoritmos.

4. Territorios rurales y pobreza digital: exclusi√≥n por dise√±o
La colonialidad algor√≠tmica tambi√©n se manifiesta a trav√©s del abandono digital. Los territorios rurales y perif√©ricos suelen quedar sistem√°ticamente fuera de los procesos de dise√±o, entrenamiento y evaluaci√≥n de los sistemas de inteligencia artificial, lo que produce tecnolog√≠as pensadas desde y para contextos urbanos. Esta exclusi√≥n no es accidental, sino el resultado de modelos de desarrollo tecnol√≥gico que priorizan la eficiencia, la escalabilidad y la rentabilidad sobre la diversidad territorial y las condiciones reales de vida de amplios sectores de la poblaci√≥n.
En el plano material, los da√±os se expresan en sistemas automatizados que asumen de manera impl√≠cita la existencia de conectividad constante, disponibilidad de dispositivos adecuados y altos niveles de alfabetizaci√≥n digital. Estas suposiciones dejan fuera a personas que no cuentan con acceso regular a internet, que comparten dispositivos, o que carecen de identificaci√≥n digital formal, convirtiendo el acceso a derechos b√°sicos en un privilegio condicionado por la infraestructura tecnol√≥gica. Asimismo, muchos modelos de IA son entrenados principalmente con datos urbanos, lo que provoca fallos sistem√°ticos cuando se aplican en contextos rurales, donde las din√°micas econ√≥micas, sociales y territoriales difieren significativamente.
En Colombia, comunidades rurales han denunciado barreras crecientes para acceder a servicios p√∫blicos que han sido digitalizados de manera acelerada. En estos casos, la automatizaci√≥n ha sustituido oficinas presenciales y mecanismos de atenci√≥n directa sin ofrecer alternativas reales y accesibles para quienes viven en zonas alejadas, profundizando la exclusi√≥n en lugar de reducirla. La digitalizaci√≥n, presentada como modernizaci√≥n, termina operando como un filtro que expulsa a quienes no encajan en el perfil del ‚Äúusuario ideal‚Äù previsto por los sistemas.
En el plano simb√≥lico, esta din√°mica refuerza la construcci√≥n de lo rural como sin√≥nimo de atraso, ineficiencia o resistencia al progreso tecnol√≥gico. Las econom√≠as locales, los saberes territoriales y las formas de vida no urbanas quedan invisibilizadas o desvalorizadas, al no ser consideradas relevantes para el dise√±o de los sistemas automatizados. De este modo, la IA contribuye a profundizar la brecha centro‚Äìperiferia, reproduciendo desigualdades hist√≥ricas bajo la apariencia de neutralidad t√©cnica y modernizaci√≥n digital.

5. Vigilancia, reconocimiento facial y control social
Uno de los campos m√°s cr√≠ticos en los que se manifiesta la colonialidad algor√≠tmica es el uso de tecnolog√≠as de reconocimiento facial y sistemas predictivos en el √°mbito de la seguridad p√∫blica. Estas herramientas suelen presentarse como soluciones objetivas y eficientes para la prevenci√≥n del delito, pero en la pr√°ctica reproducen y amplifican desigualdades raciales, sociales y territoriales preexistentes.
Casos documentados en distintos pa√≠ses de la regi√≥n muestran que los sistemas de reconocimiento facial presentan tasas de error significativamente m√°s altas en personas negras e ind√≠genas, lo que incrementa el riesgo de identificaciones incorrectas y falsas coincidencias. Estas fallas t√©cnicas, lejos de ser neutrales, se traducen en detenciones arbitrarias basadas exclusivamente en decisiones algor√≠tmicas, sin garant√≠as adecuadas de verificaci√≥n humana, debido proceso o mecanismos efectivos de reparaci√≥n.
Adem√°s, el despliegue de estas tecnolog√≠as se concentra de manera desproporcionada en barrios empobrecidos y racializados, reforzando l√≥gicas hist√≥ricas de vigilancia selectiva y control social. En este contexto, la IA colonial no solo excluye del acceso a derechos, sino que criminaliza activamente a determinados cuerpos y territorios. El da√±o producido es directo, tanto f√≠sico como psicol√≥gico, y contribuye a erosionar la confianza en las instituciones p√∫blicas, profundizando la distancia entre el Estado y las comunidades que afirma proteger.

Da√±os materiales y simb√≥licos: una distinci√≥n clave
Da√±os materiales	Da√±os simb√≥licos
Exclusi√≥n de servicios	Invisibilizaci√≥n
Vigilancia y control	Estigmatizaci√≥n
P√©rdida de derechos	Normalizaci√≥n de la desigualdad
Criminalizaci√≥n	Colonialismo epist√©mico
Ambos tipos de da√±o se refuerzan mutuamente. La IA colonial no es neutral porque opera dentro de estructuras de poder desiguales.

Cierre
Comprender a qui√©n da√±a la IA colonial es esencial para justificar la urgencia de alternativas. Estos impactos no son efectos secundarios inevitables: son el resultado de decisiones pol√≠ticas, t√©cnicas y econ√≥micas.
El siguiente paso no es solo criticar, sino transformar. En el pr√≥ximo cap√≠tulo abordaremos c√≥mo empezar a descolonizar la IA en la pr√°ctica, a trav√©s de herramientas concretas, gobernanza participativa y marcos situados desde Am√©rica Latina y el Caribe.


üü¶ Semana 4 ‚Äî ¬øQu√© proponen desde Am√©rica Latina?
Cap√≠tulo 4: Marcos, metodolog√≠as y alternativas regionales
Frente a los riesgos y da√±os de la IA colonial, Am√©rica Latina no parte de cero. Desde la regi√≥n ‚Äîy desde el Sur Global en general‚Äî existen propuestas conceptuales, metodol√≥gicas y pol√≠ticas que buscan transformar la IA en una herramienta para la justicia social, la autodeterminaci√≥n y la protecci√≥n de derechos.
Este cap√≠tulo re√∫ne marcos desarrollados desde y para la regi√≥n, que permiten pasar de la cr√≠tica a la acci√≥n: no solo mitigar da√±os, sino redistribuir poder tecnol√≥gico.

1. Epistemolog√≠as propias: pensar la IA desde el Sur
Descolonizar la inteligencia artificial implica, antes que nada, descolonizar los marcos de conocimiento que la hacen posible. Las epistemolog√≠as dominantes que hoy orientan el dise√±o y despliegue de sistemas de IA ‚Äîde corte positivista, tecnocr√°tico y euroc√©ntrico‚Äî tienden a presentar la tecnolog√≠a como neutral, universal y desvinculada de los contextos hist√≥ricos y pol√≠ticos en los que se produce. Esta mirada invisibiliza las relaciones de poder que atraviesan los procesos t√©cnicos y naturaliza modelos desarrollados desde el Norte Global como est√°ndares incuestionables.
Desde Am√©rica Latina y el Caribe, m√∫ltiples autoras, colectivos y movimientos proponen un giro epist√©mico que cuestiona de ra√≠z esta supuesta neutralidad. Parten del reconocimiento de que todo sistema t√©cnico incorpora valores, prioridades y visiones del mundo, y que la IA no es una excepci√≥n, sino una infraestructura profundamente pol√≠tica. En esta perspectiva, la tecnolog√≠a se sit√∫a en contextos hist√≥ricos marcados por el colonialismo, el extractivismo de recursos ‚Äîy hoy de datos‚Äî y la reproducci√≥n de desigualdades sociales, raciales y de g√©nero.
Este enfoque tambi√©n reivindica la legitimidad de los saberes comunitarios, ind√≠genas, feministas y populares como fuentes v√°lidas de conocimiento t√©cnico y √©tico. Lejos de ser ‚Äúcomplementos‚Äù culturales, estos saberes aportan criterios fundamentales para pensar el dise√±o, el uso y la gobernanza de la IA desde perspectivas situadas, relacionales y orientadas al cuidado colectivo. Incorporarlos implica disputar qui√©n define los problemas que la tecnolog√≠a debe resolver y bajo qu√© principios se toman esas decisiones.
Al romper con la idea de que la regi√≥n debe limitarse a adaptar modelos importados, las epistemolog√≠as propias proponen producir marcos conceptuales, metodol√≥gicos y normativos desde el Sur. Estos marcos buscan estar alineados con las realidades locales, las memorias hist√≥ricas y las necesidades sociales concretas de Am√©rica Latina, sentando las bases para una inteligencia artificial que no reproduzca la colonialidad tecnol√≥gica, sino que contribuya a procesos de autodeterminaci√≥n, justicia social y soberan√≠a regional.

2. Gobernanza de datos con enfoque comunitario: Principios CARE
Uno de los aportes m√°s significativos provenientes del Sur Global en el debate sobre descolonizaci√≥n de la inteligencia artificial es la formulaci√≥n de los Principios CARE para la gobernanza de datos, especialmente en contextos ind√≠genas, comunitarios y colectivos. Estos principios emergen como una respuesta cr√≠tica a los modelos dominantes de gesti√≥n de datos, que hist√≥ricamente han operado bajo l√≥gicas extractivas y han tratado la informaci√≥n como un recurso disponible para su explotaci√≥n, sin considerar los derechos, intereses ni cosmovisiones de las comunidades de origen.
Los Principios CARE ‚ÄîControl, Autoridad, Responsabilidad y √âtica‚Äî introducen un cambio profundo en la manera de concebir los datos y su uso. El principio de Control afirma que las comunidades deben tener capacidad real de decisi√≥n sobre c√≥mo se recolectan, utilizan, comparten y transforman sus datos, rompiendo con la pr√°ctica habitual de apropiaci√≥n unilateral por parte de Estados, empresas o centros de investigaci√≥n. La Autoridad reconoce la soberan√≠a colectiva sobre la informaci√≥n, desafiando los marcos jur√≠dicos que priorizan la propiedad individual o corporativa y negando la dimensi√≥n colectiva del conocimiento.
El principio de Responsabilidad establece que quienes acceden y utilizan datos comunitarios deben rendir cuentas por los impactos de sus pr√°cticas, tanto t√©cnicos como sociales, culturales y pol√≠ticos. Esto implica mecanismos claros de transparencia, reparaci√≥n y supervisi√≥n, y desplaza la carga √©tica desde las comunidades afectadas hacia los actores que desarrollan y despliegan tecnolog√≠as basadas en datos. Finalmente, el principio de √âtica exige que el uso de los datos responda a valores comunitarios definidos localmente y que no genere da√±o, reconociendo que la legitimidad de una pr√°ctica tecnol√≥gica no puede evaluarse √∫nicamente desde criterios t√©cnicos o de eficiencia.
En contraste con los principios FAIR ‚ÄîFindable, Accessible, Interoperable y Reusable‚Äî que priorizan la eficiencia, la apertura y la reutilizaci√≥n de los datos, los Principios CARE colocan en el centro a las personas y las comunidades. Mientras FAIR ha sido ampliamente adoptado en entornos cient√≠ficos y tecnol√≥gicos del Norte Global, su aplicaci√≥n acr√≠tica en contextos del Sur ha contribuido, en muchos casos, a nuevas formas de extracci√≥n de datos y conocimiento sin retorno ni consentimiento colectivo.
Desde una perspectiva decolonial, CARE no busca reemplazar completamente a FAIR, sino corregir sus l√≠mites y subordinarlos a un marco de derechos y autodeterminaci√≥n. La adopci√≥n de los Principios CARE obliga a repensar la innovaci√≥n tecnol√≥gica como un proceso que debe estar al servicio de las comunidades y no al rev√©s, estableciendo que la soberan√≠a de los datos y el respeto a los valores colectivos son condiciones previas para cualquier desarrollo leg√≠timo de inteligencia artificial en Am√©rica Latina y el Caribe.

3. Justicia ling√º√≠stica: IA que hable nuestras lenguas
Otro eje central de las propuestas impulsadas desde Am√©rica Latina y el Caribe es la justicia ling√º√≠stica. En la actualidad, la mayor√≠a de los sistemas de inteligencia artificial se entrenan predominantemente en ingl√©s y, en menor medida, en versiones estandarizadas del espa√±ol y el portugu√©s. Este sesgo ling√º√≠stico deja sistem√°ticamente fuera a una enorme diversidad de lenguas ind√≠genas, variantes regionales, registros locales y formas no normativas de expresi√≥n que forman parte de la vida cotidiana de millones de personas en la regi√≥n.
Esta exclusi√≥n no puede entenderse √∫nicamente como una limitaci√≥n t√©cnica o una cuesti√≥n de escala de datos. Se trata de una forma de exclusi√≥n epist√©mica y pol√≠tica: cuando una lengua no est√° presente en los datos de entrenamiento, simplemente no existe para el sistema. Como consecuencia, quienes hablan esas lenguas quedan fuera de servicios digitales, sistemas de informaci√≥n, procesos automatizados y espacios de toma de decisiones mediados por tecnolog√≠a. Al mismo tiempo, los saberes, cosmovisiones y formas de conocimiento que se transmiten a trav√©s de esas lenguas permanecen invisibilizados o son descartados como irrelevantes para la construcci√≥n de sistemas de IA.
Frente a este escenario, las propuestas desde Am√©rica Latina apuntan a revertir esta l√≥gica mediante acciones concretas. Entre ellas se encuentra el financiamiento p√∫blico y comunitario para la creaci√≥n de conjuntos de datos en lenguas ind√≠genas y minoritarias, as√≠ como el desarrollo de modelos de lenguaje locales que respondan a contextos culturales espec√≠ficos. Estas iniciativas buscan romper con la dependencia de modelos globales que reproducen jerarqu√≠as ling√º√≠sticas y abrir espacio a infraestructuras tecnol√≥gicas verdaderamente pluriling√ºes.
Asimismo, se subraya la necesidad de evitar pr√°cticas de extracci√≥n ling√º√≠stica sin retorno comunitario, en las que voces, textos y expresiones culturales son utilizados para entrenar sistemas comerciales sin consentimiento ni beneficios para las comunidades de origen. Reconocer la lengua como un derecho cultural y tecnol√≥gico implica garantizar que su uso en sistemas de IA se base en principios de respeto, reciprocidad y autodeterminaci√≥n colectiva.
En este marco, la inteligencia artificial puede transformarse en una herramienta para la revitalizaci√≥n cultural, el fortalecimiento de lenguas en riesgo y la ampliaci√≥n de derechos, en lugar de convertirse en un nuevo vector de borramiento y homogeneizaci√≥n. Descolonizar la IA desde la justicia ling√º√≠stica significa, en √∫ltima instancia, permitir que las tecnolog√≠as tambi√©n hablen nuestras lenguas y reconozcan la diversidad de mundos que estas expresan.

4. Dise√±o participativo y co-creaci√≥n: cambiar qui√©n decide
Una de las cr√≠ticas m√°s contundentes a la inteligencia artificial de car√°cter colonial es que las personas y comunidades directamente afectadas por estos sistemas rara vez participan en las decisiones sobre su dise√±o, implementaci√≥n y evaluaci√≥n. En muchos casos, la IA se introduce como una soluci√≥n t√©cnica cerrada, definida por empresas proveedoras o por equipos gubernamentales altamente especializados, sin que quienes viven las consecuencias cotidianas de esas tecnolog√≠as tengan voz ni capacidad de incidencia. Esta exclusi√≥n reproduce una l√≥gica hist√≥rica de imposici√≥n, en la que las decisiones se toman desde arriba y desde fuera, reforzando relaciones asim√©tricas de poder.
Frente a este modelo, desde Am√©rica Latina y el Caribe se propone institucionalizar metodolog√≠as que transformen radicalmente qui√©n decide y c√≥mo se toman las decisiones tecnol√≥gicas. Entre estas propuestas destacan el dise√±o participativo, la co-creaci√≥n con comunidades, los laboratorios ciudadanos y los enfoques de innovaci√≥n p√∫blica abierta. Estas metodolog√≠as parten del reconocimiento de que el conocimiento relevante para dise√±ar sistemas de IA no reside √∫nicamente en la experticia t√©cnica, sino tambi√©n en las experiencias, saberes y pr√°cticas de las personas que interact√∫an con esos sistemas en contextos concretos.
La co-creaci√≥n, en este sentido, no puede reducirse a instancias de consulta simb√≥lica, talleres informativos o procesos de validaci√≥n ex post. Implica un cambio estructural en la distribuci√≥n del poder dentro del ciclo de vida de la tecnolog√≠a. Las comunidades, organizaciones sociales y personas usuarias deben participar desde el inicio en la definici√≥n de los problemas que se busca resolver, evitando que la IA se aplique a cuestiones mal formuladas o desconectadas de las necesidades reales del territorio. Asimismo, deben involucrarse en el dise√±o de indicadores, m√©tricas y criterios de evaluaci√≥n, de modo que los sistemas respondan a valores sociales y no solo a par√°metros de eficiencia t√©cnica.
Un elemento central de este enfoque es el reconocimiento del derecho a modificar, detener o vetar sistemas que generen da√±o, exclusi√≥n o efectos no deseados. La participaci√≥n significativa supone que las comunidades tengan poder real de decisi√≥n, y no solo un rol consultivo, as√≠ como un lugar en los procesos de evaluaci√≥n continua, auditor√≠a y ajuste de los sistemas a lo largo del tiempo. Esto permite identificar impactos imprevistos, corregir sesgos emergentes y garantizar que la tecnolog√≠a se mantenga alineada con los derechos y prioridades colectivas.
Al adoptar estas metodolog√≠as, se desplaza el modelo vertical tradicional ‚Äîbasado en la relaci√≥n Estado‚Äìempresa‚Äìusuario‚Äî hacia formas de gobernanza distribuida, m√°s democr√°ticas y situadas. En lugar de tecnolog√≠as impuestas, se construyen sistemas negociados, contextualizados y corresponsables, donde la inteligencia artificial deja de ser una herramienta de control y pasa a convertirse en un proceso colectivo orientado al bienestar, la justicia social y la autodeterminaci√≥n tecnol√≥gica.

5. Evaluaciones de Impacto Algor√≠tmico situadas
Para traducir los principios de justicia algor√≠tmica y descolonizaci√≥n de la tecnolog√≠a en pol√≠ticas p√∫blicas concretas, diversos actores en Am√©rica Latina y el Caribe han comenzado a experimentar con Evaluaciones de Impacto Algor√≠tmico (AIA) adaptadas a los contextos sociales, pol√≠ticos y territoriales de la regi√≥n. Estas evaluaciones surgen como una respuesta cr√≠tica a los modelos importados que, en muchos casos, se limitan a listas de verificaci√≥n t√©cnicas o a marcos abstractos de cumplimiento, sin atender a las desigualdades estructurales propias de los pa√≠ses del Sur Global.
A diferencia de estos enfoques estandarizados, las AIA situadas buscan analizar los efectos reales de los sistemas de inteligencia artificial sobre distintos grupos sociales, considerando de manera expl√≠cita variables como g√©nero, raza, clase, territorio y orientaci√≥n sexual e identidad de g√©nero (OSIG). Este enfoque permite visibilizar impactos diferenciados que suelen quedar ocultos en evaluaciones gen√©ricas, as√≠ como identificar riesgos espec√≠ficos de exclusi√≥n, vigilancia intensificada y reproducci√≥n de discriminaciones hist√≥ricas. La evaluaci√≥n deja de centrarse √∫nicamente en el rendimiento t√©cnico del sistema y se desplaza hacia sus consecuencias sociales y pol√≠ticas.
Un componente central de las AIA situadas es la exigencia de explicabilidad y auditabilidad de los sistemas automatizados. Esto implica que las decisiones algor√≠tmicas deben poder ser comprendidas, cuestionadas y revisadas por las personas afectadas, por organismos de control y por actores independientes. La opacidad t√©cnica, com√∫n en muchos sistemas comerciales o propietarios, se reconoce as√≠ como un riesgo democr√°tico y de derechos humanos que debe ser activamente mitigado mediante obligaciones regulatorias claras.
Asimismo, estas evaluaciones incorporan la participaci√≥n de la sociedad civil organizada, la academia cr√≠tica y, en algunos casos, de comunidades directamente afectadas por los sistemas evaluados. Esta participaci√≥n ampl√≠a el marco de an√°lisis, introduce perspectivas no t√©cnicas y fortalece la legitimidad de los procesos de evaluaci√≥n. En la regi√≥n, organismos como el Banco Interamericano de Desarrollo (BID), en colaboraci√≥n con laboratorios ciudadanos y organizaciones sociales, han impulsado herramientas y metodolog√≠as iniciales de AIA que pueden ser apropiadas, adaptadas y fortalecidas por los Estados nacionales y locales.
La clave de las Evaluaciones de Impacto Algor√≠tmico situadas no reside √∫nicamente en la producci√≥n de diagn√≥sticos, sino en la capacidad efectiva de actuar sobre sus resultados. Evaluar sin consecuencias reproduce una l√≥gica de simulaci√≥n √©tica. Por el contrario, un enfoque comprometido con los derechos humanos exige que los hallazgos de las AIA habiliten decisiones concretas, incluyendo la modificaci√≥n, suspensi√≥n o prohibici√≥n de sistemas que vulneren derechos fundamentales. De este modo, las AIA se consolidan como una herramienta pol√≠tica y regulatoria esencial para disputar el sentido y los l√≠mites de la inteligencia artificial en la regi√≥n.

6. Alternativas en construcci√≥n: un campo en disputa
Las propuestas desde Am√©rica Latina demuestran que el futuro de la IA no est√° cerrado. Es un campo Las propuestas que emergen desde Am√©rica Latina y el Caribe muestran con claridad que el futuro de la inteligencia artificial no est√° predeterminado ni cerrado. Lejos de ser una evoluci√≥n t√©cnica inevitable, la IA constituye un campo en disputa pol√≠tica, social, t√©cnica y cultural, en el que se enfrentan distintos proyectos de sociedad, modelos de desarrollo y concepciones sobre el poder y los derechos. Reconocer esta disputa es fundamental para desmontar la idea de que la regi√≥n solo puede adoptar pasivamente tecnolog√≠as dise√±adas en otros contextos.
En este escenario, comienzan a consolidarse alternativas que desaf√≠an la hegemon√≠a tecnol√≥gica del Norte Global. Entre ellas se encuentran marcos regulatorios que incorporan expl√≠citamente un enfoque de derechos humanos, capaces de establecer l√≠mites claros al uso de sistemas automatizados en √°mbitos sensibles como la seguridad, el bienestar social o la justicia. Estos marcos buscan garantizar la transparencia, la rendici√≥n de cuentas y la protecci√≥n de derechos fundamentales, situando a las personas y comunidades en el centro de la regulaci√≥n tecnol√≥gica.
Paralelamente, se han fortalecido redes de investigaci√≥n feminista, decolonial y cr√≠tica que producen conocimiento situado sobre inteligencia artificial desde el Sur. Estas redes cuestionan los supuestos universales de la tecnolog√≠a, visibilizan los impactos diferenciados de los sistemas automatizados y proponen metodolog√≠as alternativas para su dise√±o, evaluaci√≥n y gobernanza. Su trabajo demuestra que es posible generar teor√≠a, evidencia y pr√°ctica tecnol√≥gica desde marcos epistemol√≥gicos propios.
Otra l√≠nea clave de acci√≥n es el impulso de infraestructuras p√∫blicas y comunitarias, orientadas a reducir la dependencia de plataformas y proveedores privados transnacionales. Estas iniciativas buscan recuperar la capacidad estatal y colectiva para gestionar datos, sistemas y redes cr√≠ticas, fortaleciendo la soberan√≠a tecnol√≥gica y ampliando el acceso equitativo a servicios digitales. En este proceso, las alianzas entre Estado, sociedad civil y academia cr√≠tica resultan fundamentales para construir capacidades sostenibles y democr√°ticas.
Estas alternativas no persiguen ‚Äúalcanzar‚Äù o imitar al Norte Global dentro de las mismas l√≥gicas de competencia y extractivismo tecnol√≥gico. Por el contrario, su objetivo es descentrar ese modelo, cuestionar su pretendida universalidad y abrir espacio a m√∫ltiples formas de pensar, dise√±ar y gobernar la inteligencia artificial. En este sentido, descolonizar la IA no implica rechazar la tecnolog√≠a, sino disputar activamente su sentido, sus usos y sus beneficiarios desde las realidades y aspiraciones del Sur Global.

Cierre 
Descolonizar la inteligencia artificial no significa rechazar la tecnolog√≠a ni situarse en una postura de negaci√≥n frente a la innovaci√≥n. Por el contrario, implica reapropiarla de manera cr√≠tica, pol√≠tica y colectiva, cuestionando qui√©n la dise√±a, para qu√© fines se utiliza y a costa de qui√©nes se despliega. Las propuestas que emergen desde Am√©rica Latina y el Caribe demuestran que existen alternativas concretas para construir sistemas de IA alineados con los derechos humanos, las realidades territoriales y las necesidades sociales de la regi√≥n.
Estas propuestas apuntan a desarrollar tecnolog√≠as que respeten los derechos fundamentales, que reflejen la diversidad cultural, ling√º√≠stica y social de los contextos en los que operan, y que contribuyan a redistribuir el poder actualmente concentrado en pocos actores tecnol√≥gicos globales. En lugar de reproducir jerarqu√≠as hist√≥ricas, una IA situada y decolonial puede convertirse en una herramienta para fortalecer la democracia, ampliar la participaci√≥n ciudadana y reforzar la capacidad de decisi√≥n de comunidades, Estados y organizaciones sociales.
Este cierre sintetiza el recorrido conceptual del documento y, al mismo tiempo, abre una invitaci√≥n clara y urgente: pasar de la reflexi√≥n a la acci√≥n colectiva. Descolonizar la IA requiere transformar estos marcos te√≥ricos en pr√°cticas concretas, pol√≠ticas p√∫blicas, procesos de incidencia y formas de organizaci√≥n que disputen el rumbo de la tecnolog√≠a. El desaf√≠o no es menor, pero tampoco lo es la oportunidad de construir una inteligencia artificial al servicio de la justicia social, la autodeterminaci√≥n y el bien com√∫n.

üü™ Semana 5 ‚Äî ¬øC√≥mo empezar a descolonizar la IA?
Cap√≠tulo 5: De la cr√≠tica a la acci√≥n
Despu√©s del diagn√≥stico y de la revisi√≥n de marcos y propuestas regionales, la pregunta inevitable es c√≥mo empezar. Descolonizar la IA no requiere esperar a tecnolog√≠as futuras ni marcos ideales: implica decisiones concretas en el presente, en proyectos, pol√≠ticas p√∫blicas, organizaciones y comunidades.
Este cap√≠tulo propone un conjunto de entradas pr√°cticas para pasar de la cr√≠tica a la acci√≥n situada.

1. Qu√© mirar, qu√© preguntar, qu√© medir
El primer paso para descolonizar la IA es cambiar las preguntas. La evaluaci√≥n de un sistema no puede limitarse a precisi√≥n, eficiencia o escalabilidad.
Algunas preguntas clave desde una perspectiva decolonial son:
Qu√© mirar
    ‚Ä¢ ¬øQui√©n dise√±√≥ el sistema y desde d√≥nde?
    ‚Ä¢ ¬øQu√© problema dice resolver y qui√©n lo defini√≥?
    ‚Ä¢ ¬øEn qu√© contextos sociales, culturales y territoriales se implementa?
Qu√© preguntar
    ‚Ä¢ ¬øA qui√©n beneficia realmente este sistema?
    ‚Ä¢ ¬øQui√©n asume los riesgos y los errores?
    ‚Ä¢ ¬øQu√© grupos quedan fuera o son mal representados?
    ‚Ä¢ ¬øExiste consentimiento informado, colectivo y continuo?
Qu√© medir
    ‚Ä¢ Impactos diferenciados por g√©nero, raza, clase, territorio y OSIG.
    ‚Ä¢ Riesgos de exclusi√≥n, vigilancia y estigmatizaci√≥n.
    ‚Ä¢ Capacidad real de las personas para impugnar decisiones algor√≠tmicas.
    ‚Ä¢ Da√±os materiales (acceso a servicios, ingresos, derechos) y simb√≥licos (dignidad, reconocimiento, identidad).
Medir sin este enfoque reproduce la colonialidad t√©cnica; medir con enfoque situado la cuestiona.

2. Gobernanza distribuida: decidir con, no sobre
Uno de los pilares centrales de la inteligencia artificial colonial es la concentraci√≥n del poder de decisi√≥n en un n√∫mero reducido de actores, principalmente grandes empresas tecnol√≥gicas, √©lites t√©cnicas especializadas y agencias estatales que operan de manera cerrada y poco transparente. En este modelo, las decisiones sobre qu√© sistemas se desarrollan, c√≥mo funcionan y a qui√©nes afectan se toman sin la participaci√≥n de las personas y comunidades que viven sus consecuencias cotidianas.
Descolonizar la IA implica avanzar hacia modelos de gobernanza distribuida que rompan con esta l√≥gica vertical y excluyente. Esto supone incorporar de manera formal y sostenida a la sociedad civil, a las comunidades afectadas y a organizaciones sociales en los procesos de dise√±o, implementaci√≥n y evaluaci√≥n de sistemas algor√≠tmicos. No se trata de consultas puntuales o simb√≥licas, sino de la creaci√≥n de espacios permanentes de deliberaci√≥n con capacidad real de incidencia.
La gobernanza distribuida tambi√©n requiere garantizar el acceso p√∫blico a informaci√≥n clara y comprensible sobre los sistemas de inteligencia artificial en uso, sus objetivos, sus criterios de decisi√≥n y los datos que los alimentan. Sin transparencia no es posible ejercer control social ni exigir responsabilidades frente a posibles da√±os. A ello se suman mecanismos de rendici√≥n de cuentas claros, efectivos y vinculantes, que permitan corregir, suspender o retirar sistemas que vulneren derechos.
En este enfoque, la legitimidad tecnol√≥gica no se deriva √∫nicamente de la eficiencia t√©cnica o la innovaci√≥n, sino de su anclaje democr√°tico. Reconocer que la tecnolog√≠a es un asunto pol√≠tico implica aceptar que sin participaci√≥n real no puede haber justicia algor√≠tmica. La gobernanza distribuida, por tanto, no es un complemento opcional, sino una condici√≥n fundamental para descolonizar la inteligencia artificial y devolver el poder de decisi√≥n a quienes hist√≥ricamente han sido excluidos de √©l.

3. No-delegaci√≥n algor√≠tmica: HITL real, no simb√≥lico
La consigna Human in the Loop (HITL) suele presentarse como una garant√≠a √©tica en el uso de sistemas de inteligencia artificial. Sin embargo, en la pr√°ctica, este principio muchas veces se aplica de forma meramente simb√≥lica. En numerosos contextos, la intervenci√≥n humana se limita a validar o ejecutar decisiones que ya han sido previamente determinadas por el sistema algor√≠tmico, sin margen real para el cuestionamiento o la correcci√≥n.
Desde una perspectiva decolonial, se propone avanzar hacia un principio m√°s robusto: la no-delegaci√≥n algor√≠tmica. Este enfoque sostiene que, especialmente en contextos de alto impacto sobre derechos y condiciones de vida, las decisiones no deben ser delegadas de facto a sistemas automatizados. La responsabilidad final debe recaer siempre en una persona identificable, con capacidad y autoridad para evaluar cr√≠ticamente la recomendaci√≥n algor√≠tmica.
Un enfoque de no-delegaci√≥n algor√≠tmica implica que la intervenci√≥n humana no sea un simple tr√°mite administrativo, sino una instancia sustantiva de deliberaci√≥n. La persona responsable debe contar con informaci√≥n suficiente para comprender c√≥mo el sistema lleg√≥ a una determinada recomendaci√≥n, y debe tener la posibilidad efectiva de cuestionarla, modificarla o revertirla cuando sea necesario. Esto requiere trazabilidad clara de los procesos de decisi√≥n, as√≠ como sistemas dise√±ados para facilitar la comprensi√≥n y el control humano.
Asimismo, este principio rechaza la idea de que el error algor√≠tmico pueda diluir o desplazar las responsabilidades institucionales. Cuando un sistema falla o produce da√±o, la rendici√≥n de cuentas no puede atribuirse a una ‚Äúcaja negra‚Äù t√©cnica, sino que debe recaer en las instituciones y actores que decidieron su adopci√≥n, dise√±o y uso. La no-delegaci√≥n algor√≠tmica resulta especialmente clave en √°mbitos como la justicia, la salud, el bienestar social, la migraci√≥n y la seguridad, donde las decisiones automatizadas pueden tener consecuencias profundas, irreversibles y desiguales sobre las personas y las comunidades.

4. Soberan√≠a sobre infraestructura: m√°s all√° del software
Descolonizar la inteligencia artificial no se limita al an√°lisis del software, los algoritmos o los datos. La infraestructura tecnol√≥gica ‚Äîservidores, centros de datos, servicios en la nube y redes de telecomunicaciones‚Äî constituye un eje central de poder que condiciona de manera profunda c√≥mo se dise√±an, operan y gobiernan los sistemas de IA. Ignorar esta dimensi√≥n implica dejar intactas las bases materiales sobre las que se reproduce la colonialidad digital.
En Am√©rica Latina y el Caribe, la dependencia casi total de proveedores de infraestructura del Norte Global genera m√∫ltiples formas de vulnerabilidad. Esta dependencia expone a los Estados y a las instituciones p√∫blicas a riesgos pol√≠ticos y econ√≥micos, limita el control efectivo sobre datos sensibles y estrat√©gicos, y dificulta o directamente impide la auditor√≠a plena de los sistemas que procesan informaci√≥n cr√≠tica. Adem√°s, concentra decisiones t√©cnicas y comerciales fuera del territorio, debilitando la soberan√≠a digital y la capacidad de respuesta frente a conflictos, cambios regulatorios o interrupciones del servicio.
La falta de control infraestructural tambi√©n tiene implicaciones directas para los derechos humanos. Cuando los datos de poblaciones enteras se almacenan y procesan en nubes extranjeras, bajo marcos legales ajenos y contratos opacos, se reduce la posibilidad de garantizar privacidad, seguridad y uso leg√≠timo de la informaci√≥n. En este escenario, incluso los marcos normativos m√°s avanzados pueden volverse ineficaces si no cuentan con respaldo material y t√©cnico propio.
Frente a este panorama, las propuestas desde la regi√≥n apuntan a una estrategia activa de recuperaci√≥n y fortalecimiento de la soberan√≠a sobre la infraestructura digital. Esto incluye invertir en infraestructura p√∫blica y regional, establecer condiciones claras y exigibles para los proveedores externos, priorizar tecnolog√≠as abiertas y auditables que permitan mayor control y transparencia, y desarrollar capacidades t√©cnicas locales que reduzcan la dependencia estructural. Sin una base infraestructural soberana, la descolonizaci√≥n de la IA queda incompleta, ya que el poder real contin√∫a concentrado en actores externos, m√°s all√° de cualquier ajuste en el discurso o en el dise√±o de los sistemas.

5. Rol del Estado y la sociedad civil
La descolonizaci√≥n de la inteligencia artificial no puede ser asumida por un solo actor ni reducida a iniciativas aisladas. Se trata de un proceso estructural que requiere la acci√≥n coordinada ‚Äîy en tensi√≥n productiva‚Äî entre el Estado, la sociedad civil, las comunidades afectadas y otros actores del ecosistema tecnol√≥gico. Sin esta articulaci√≥n, los esfuerzos por transformar la IA corren el riesgo de quedarse en declaraciones simb√≥licas sin impacto real.
El rol del Estado es central. Corresponde a las instituciones p√∫blicas regular el desarrollo y uso de la IA desde un enfoque de derechos humanos, estableciendo l√≠mites claros a los sistemas que afectan derechos fundamentales. Esto implica exigir evaluaciones de impacto algor√≠tmico situadas antes de la adopci√≥n de tecnolog√≠as de alto riesgo, garantizar la transparencia y el acceso p√∫blico a la informaci√≥n sobre los sistemas utilizados, y asegurar mecanismos efectivos de rendici√≥n de cuentas. Asimismo, el Estado tiene la responsabilidad de proteger de manera prioritaria a las poblaciones hist√≥ricamente vulneradas, evitando que la automatizaci√≥n reproduzca o profundice desigualdades existentes.
La sociedad civil, por su parte, cumple un papel irremplazable como contrapeso y como fuente de conocimiento situado. Organizaciones sociales, colectivos comunitarios, movimientos feministas, ind√≠genas y de derechos humanos monitorean el uso de tecnolog√≠as, documentan abusos, denuncian impactos negativos y proponen alternativas basadas en la experiencia directa de quienes viven las consecuencias de la automatizaci√≥n. Su participaci√≥n no se limita a la cr√≠tica: tambi√©n aportan metodolog√≠as de co-creaci√≥n, auditor√≠a social y dise√±o participativo que permiten construir sistemas m√°s justos y contextualizados.
La relaci√≥n entre el Estado y la sociedad civil no debe ser jer√°rquica ni instrumental. Una descolonizaci√≥n efectiva de la IA requiere una relaci√≥n colaborativa, pero tambi√©n tensionada, en la que existan espacios de di√°logo, desacuerdo y vigilancia mutua. Esta din√°mica es parte constitutiva de una democracia saludable y resulta indispensable para garantizar que la transformaci√≥n tecnol√≥gica no se realice a espaldas de la ciudadan√≠a, sino como un proceso colectivo orientado a la justicia algor√≠tmica y la autodeterminaci√≥n regional.

6. Conexi√≥n con activismo y trabajo de Arrecife
Descolonizar la inteligencia artificial es una tarea inseparable del activismo por los derechos humanos. No se trata √∫nicamente de optimizar sistemas t√©cnicos o corregir sesgos puntuales, sino de disputar el sentido pol√≠tico de tecnolog√≠as que hoy intervienen en la gesti√≥n de cuerpos, territorios, identidades y derechos. La IA, cuando se despliega sin control democr√°tico ni enfoque de justicia social, puede convertirse en una nueva herramienta de vigilancia, exclusi√≥n y disciplinamiento. Frente a ello, la descolonizaci√≥n aparece como una forma de resistencia y de defensa activa de la dignidad humana.
En este marco, el trabajo de Arrecife se sit√∫a expl√≠citamente en la intersecci√≥n entre datos, tecnolog√≠a y derechos humanos, con una mirada anclada en las realidades de Am√©rica Latina. La organizaci√≥n apuesta por la producci√≥n de conocimiento cr√≠tico desde la regi√≥n, cuestionando marcos importados y visibilizando c√≥mo la colonialidad tecnol√≥gica reproduce desigualdades hist√≥ricas. Este enfoque no es meramente acad√©mico: busca generar evidencia √∫til para la incidencia pol√≠tica, la toma de decisiones p√∫blicas y la construcci√≥n de alternativas concretas frente al uso indiscriminado de tecnolog√≠as automatizadas.
Arrecife articula investigaci√≥n, an√°lisis de datos y trabajo territorial para acompa√±ar a organizaciones sociales, colectivas y comunidades que enfrentan impactos reales de la automatizaci√≥n. A trav√©s del fortalecimiento de capacidades, la sistematizaci√≥n de casos y la creaci√≥n de espacios de di√°logo entre actores diversos, la organizaci√≥n contribuye a que las discusiones sobre IA no queden restringidas a √©lites t√©cnicas, sino que incorporen voces hist√≥ricamente excluidas de los procesos de dise√±o y gobernanza tecnol√≥gica.
En √∫ltima instancia, la descolonizaci√≥n de la IA es una lucha por el derecho a decidir. Decidir qu√© tecnolog√≠as se adoptan, bajo qu√© condiciones, con qu√© datos y con qu√© l√≠mites. Decidir qui√©n participa en esas decisiones y qui√©n asume responsabilidades cuando los sistemas fallan o generan da√±o. Desde esta perspectiva, la IA no es solo un campo t√©cnico, sino un terreno pol√≠tico en disputa, y el trabajo de Arrecife se inscribe en ese esfuerzo colectivo por construir tecnolog√≠as que est√©n al servicio de la vida, los derechos y la autodeterminaci√≥n de nuestras comunidades.

Cierre
Pasar de la cr√≠tica a la acci√≥n implica asumir que la IA no es inevitable ni neutral. Es un campo en disputa.
Descolonizar la IA es un proceso continuo que requiere:
    ‚Ä¢ Preguntar distinto.
    ‚Ä¢ Dise√±ar distinto.
    ‚Ä¢ Gobernar distinto.
    ‚Ä¢ Y, sobre todo, hacerlo desde el Sur, con el Sur y para el Sur.


üîö Cierre
Descolonizar la IA no es rechazar la tecnolog√≠a, es disputar su sentido
Descolonizar la inteligencia artificial no significa oponerse al desarrollo tecnol√≥gico ni idealizar un pasado sin herramientas digitales. Significa reconocer que la tecnolog√≠a nunca es neutral, que siempre expresa relaciones de poder, y que hoy la IA est√° siendo dise√±ada, desplegada y gobernada desde marcos que reproducen desigualdades hist√≥ricas.
La disputa no es contra la IA en s√≠, sino contra un modelo de IA: extractivo, centralizado, opaco y desconectado de las realidades sociales, culturales y territoriales de Am√©rica Latina y el Caribe.
Frente a esto, descolonizar la IA implica reclamar el derecho a decidir:
    ‚Ä¢ qu√© problemas se resuelven con tecnolog√≠a,
    ‚Ä¢ c√≥mo se dise√±an los sistemas,
    ‚Ä¢ qu√© datos se usan,
    ‚Ä¢ qui√©n asume los riesgos,
    ‚Ä¢ y qui√©n se beneficia de sus resultados.
Es una disputa por el sentido, por la orientaci√≥n √©tica y pol√≠tica de la tecnolog√≠a, y por su capacidad de servir a la justicia social y no a la profundizaci√≥n de la exclusi√≥n.

üì£ Llamado a la acci√≥n
Este documento no busca ser un punto de llegada, sino un punto de partida. Descolonizar la IA es un proceso colectivo que requiere la participaci√≥n de:
    ‚Ä¢ organizaciones de derechos humanos,
    ‚Ä¢ movimientos feministas y LGBTIQ+,
    ‚Ä¢ pueblos ind√≠genas y comunidades afrodescendientes,
    ‚Ä¢ acad√©micas y acad√©micos del Sur Global,
    ‚Ä¢ responsables de pol√≠ticas p√∫blicas,
    ‚Ä¢ y personas t√©cnicas dispuestas a cuestionar las l√≥gicas dominantes.
Invitamos a:
    ‚Ä¢ usar este marco para analizar proyectos, pol√≠ticas y sistemas algor√≠tmicos;
    ‚Ä¢ adaptarlo a contextos locales y sectoriales;
    ‚Ä¢ contribuir con casos, lecturas, metodolog√≠as y experiencias desde la regi√≥n;
    ‚Ä¢ exigir gobernanza democr√°tica, transparencia y rendici√≥n de cuentas en el uso de IA.

üîó Recursos y continuidad
üìÇ Repositorio GitHub ‚Äî Descolonizar la IA (ALC)
Aqu√≠ encontrar√°s el documento completo, lecturas clave, recursos por tema y materiales abiertos para investigaci√≥n, incidencia y formaci√≥n.
üëâ (agregar enlace al repo)
üåê Linktree de Arrecife
Accede a nuestras redes, publicaciones, recursos y pr√≥ximos contenidos de la campa√±a.
üëâ (agregar enlace al Linktree)

üìö Bibliograf√≠a (Fuentes utilizadas para el an√°lisis)
Esta bibliograf√≠a re√∫ne textos acad√©micos, informes, art√≠culos y documentos producidos desde Am√©rica Latina y el Caribe ‚Äîo centrados expl√≠citamente en la regi√≥n‚Äî que fundamentan el an√°lisis sobre colonialidad tecnol√≥gica, inteligencia artificial, justicia algor√≠tmica y descolonizaci√≥n de la tecnolog√≠a.
Colonialidad, poder y tecnolog√≠a
    ‚Ä¢ Ricaurte, Paola (2019). Data epistemologies, the coloniality of power, and resistance.
An√°lisis clave sobre c√≥mo los datos y la IA reproducen estructuras coloniales y c√≥mo pensar alternativas desde el Sur Global.
    ‚Ä¢ Milan, Stefania & Trer√©, Emiliano (2020). The rise of the data poor: The COVID-19 pandemic seen from the margins.
Examina exclusiones digitales y desigualdades estructurales en contextos latinoamericanos.
    ‚Ä¢ Trer√©, Emiliano (2022). Hybrid media activism: Ecologies, imaginaries, algorithms.
Aborda el rol de infraestructuras digitales y algoritmos en disputas pol√≠ticas contempor√°neas.
    ‚Ä¢ Quijano, An√≠bal (2000). Colonialidad del poder, eurocentrismo y Am√©rica Latina.
Texto fundamental para comprender la colonialidad como estructura persistente que atraviesa tecnolog√≠a y conocimiento.

Feminismos, datos y justicia algor√≠tmica
    ‚Ä¢ D‚ÄôIgnazio, Catherine & Klein, Lauren (2020). Data Feminism.
Marco clave utilizado cr√≠ticamente desde Am√©rica Latina para repensar poder, datos y tecnolog√≠a.
    ‚Ä¢ Espinosa Mi√±oso, Yuderkys (2014). Tejiendo de otro modo: Feminismo, epistemolog√≠a y apuestas descoloniales en Abya Yala.
Aporta una cr√≠tica feminista decolonial fundamental para pensar tecnolog√≠a y conocimiento situado.
    ‚Ä¢ DataG√©nero (Argentina). Informes sobre g√©nero, datos y pol√≠ticas p√∫blicas.
Documentaci√≥n emp√≠rica sobre exclusi√≥n algor√≠tmica y sesgos en sistemas estatales.

Inteligencia artificial, derechos humanos y pol√≠ticas p√∫blicas
    ‚Ä¢ BID ‚Äì Banco Interamericano de Desarrollo (2020‚Äì2023).
Algorithmic Impact Assessment Toolkit y documentos sobre IA y sector p√∫blico en Am√©rica Latina.
    ‚Ä¢ Ramos, Luc√≠a; Segura, Mar√≠a; otros (varios).
Estudios sobre automatizaci√≥n del bienestar social, g√©nero y exclusi√≥n en Am√©rica Latina.
    ‚Ä¢ Global Voices / Entrevistas regionales
Testimonios y an√°lisis cr√≠ticos sobre colonialismo digital, lenguas ind√≠genas y tecnolog√≠as emergentes.

Gobernanza de datos y justicia ling√º√≠stica
    ‚Ä¢ Principios CARE para la Gobernanza de Datos Ind√≠genas
Collective Benefit, Authority to Control, Responsibility, Ethics.
    ‚Ä¢ Sursiendo (M√©xico). Colonialismo digital y tecnolog√≠as comunitarias.
Experiencias pr√°cticas de soberan√≠a tecnol√≥gica y datos comunitarios.
    ‚Ä¢ Laboratorio El Rule (M√©xico).
Innovaci√≥n c√≠vica, tecnolog√≠a situada y participaci√≥n ciudadana.

üåé Autoras, autores y organizaciones del Sur Global y el Caribe recomendadas
Las siguientes autoras, autores y colectivos producen pensamiento cr√≠tico, investigaci√≥n aplicada y acci√≥n pol√≠tica desde Am√©rica Latina y el Caribe, contribuyendo activamente a la descolonizaci√≥n de la tecnolog√≠a y la inteligencia artificial.
Autoras y autores
    ‚Ä¢ Paola Ricaurte (M√©xico)
    ‚Ä¢ Yuderkys Espinosa Mi√±oso (Rep√∫blica Dominicana / Caribe)
    ‚Ä¢ Rita Segato (Argentina / Brasil)
    ‚Ä¢ An√≠bal Quijano (Per√∫)
    ‚Ä¢ Silvia Rivera Cusicanqui (Bolivia)
    ‚Ä¢ Emiliano Trer√© (M√©xico / Ecuador)
    ‚Ä¢ Mar√≠a Paz Canales (Chile)
    ‚Ä¢ Joana Varon (Brasil)
    ‚Ä¢ Luc√≠a Ramos (Argentina)

Organizaciones y colectivos
    ‚Ä¢ DataG√©nero (Argentina)
    ‚Ä¢ Sursiendo (M√©xico)
    ‚Ä¢ Laboratorio El Rule (M√©xico)
    ‚Ä¢ Derechos Digitales (Chile / ALC)
    ‚Ä¢ Fundaci√≥n Karisma (Colombia)
    ‚Ä¢ Coding Rights (Brasil)
    ‚Ä¢ Instituto Nupef (Brasil)
    ‚Ä¢ Hiperderecho (Per√∫)

Redes y espacios regionales
    ‚Ä¢ Feminist AI Research Network (ALC)
    ‚Ä¢ Red de Datos Feministas en Am√©rica Latina
    ‚Ä¢ Coaliciones por la Justicia Algor√≠tmica en ALC
    ‚Ä¢ Iniciativas de lenguas ind√≠genas y tecnolog√≠a comunitaria
Esta bibliograf√≠a no pretende ser exhaustiva. Es una invitaci√≥n abierta a seguir ampliando, citando y construyendo conocimiento colectivo desde Am√©rica Latina y el Caribe, reconociendo que la descolonizaci√≥n de la inteligencia artificial es un proceso vivo, situado y en disputa.
